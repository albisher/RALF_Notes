import re
import yaml
from typing import Dict, Any, List, Tuple
import logging # ADD THIS IMPORT

logger = logging.getLogger(__name__) # ADD THIS LINE

class TextParser:
    """
    Parses the LLM's structured text output into a dictionary.
    """

    def parse(self, raw_text: str, filename: str) -> Dict[str, Any]:
        """
        Parses the raw text and returns a dictionary.
        The filename is provided as a separate argument, as the LLM no longer provides it.
        """
        logger.debug("Parsing raw text for file: %s", filename) # ADD LOGGING
        sections = self._split_into_sections(raw_text)
        
        parsed_data = {
            "filename": filename, # Directly assign filename
            "summary": self._parse_summary(sections.get("SUMMARY", "")),
            "tags": self._parse_tags(sections.get("TAGS", "")),
            "type": self._parse_type(sections.get("TYPE", "")),
            "key_functions": self._parse_key_functions(sections.get("KEY_FUNCTIONS", "")),
            "details": self._parse_details(sections.get("DETAILS", "")),
            "dependencies": self._parse_dependencies(sections.get("DEPENDENCIES", "")),
            "usage": self._parse_usage(sections.get("USAGE", "")),
            "related": self._parse_related(sections.get("RELATED", "")),
            "callouts": self._parse_callouts(sections.get("CALLOUTS", "")),
        }
        logger.debug("Finished parsing raw text for file: %s", filename) # ADD LOGGING
        return parsed_data

    def parse_markdown(self, markdown_content: str) -> Dict[str, Any]:
        """
        Parses a Markdown document (typically generated by NoteFormatter)
        back into a structured dictionary.
        """
        logger.debug("Parsing markdown content.") # ADD LOGGING
        data = {}
        
        # 1. Extract YAML Frontmatter
        frontmatter_match = re.match(r'^---\s*\n(.*?)\n---\s*\n', markdown_content, re.DOTALL)
        if frontmatter_match:
            frontmatter_str = frontmatter_match.group(1)
            try:
                frontmatter = yaml.safe_load(frontmatter_str)
                if frontmatter:
                    tags_str = frontmatter.get('tags', '')
                    data['tags'] = self._parse_tags_from_frontmatter(tags_str)
                    data['type'] = frontmatter.get('type', '')
                    logger.debug("Extracted frontmatter: tags=%s, type=%s", data['tags'], data['type']) # ADD LOGGING
            except yaml.YAMLError as e:
                logger.warning(f"Error parsing YAML frontmatter: {e}") # CHANGED FROM PRINT
            markdown_content = markdown_content[frontmatter_match.end():]

        # 2. Extract H1 Header as filename
        h1_match = re.match(r'^\s*#\s*(.*?)\s*\n', markdown_content)
        if h1_match:
            data['filename'] = h1_match.group(1).strip()
            logger.debug("Extracted filename from H1: %s", data['filename']) # ADD LOGGING
            markdown_content = markdown_content[h1_match.end():]
        
        # 3. Extract other sections using a similar approach to _split_into_sections
        # Modify the pattern to look for ## headers instead of ### for main sections
        sections_dict = {}
        # Pattern for ## headers and ### headers for key functions
        pattern = r'^\s*##\s*(?P<name>[A-Za-z\s]+)\s*\n(?P<content>.*?)(?=\n\s*##\s*|\Z)'
        matches = re.finditer(pattern, markdown_content, re.DOTALL | re.MULTILINE)
        for match in matches:
            name = match.group('name').strip().replace(' ', '_').upper()
            content = match.group('content').strip()
            sections_dict[name] = content
            logger.debug("Extracted markdown section: %s", name) # ADD LOGGING
        
        # Manually extract Key Functions because they use ### headers within ## Key Functions
        key_functions_content = sections_dict.get("KEY_FUNCTIONS", "")
        if key_functions_content:
            data["key_functions"] = self._parse_key_functions(key_functions_content)
            logger.debug("Parsed key functions.") # ADD LOGGING

        data["summary"] = sections_dict.get("SUMMARY", "")
        data["details"] = sections_dict.get("DETAILS", "")
        data["usage"] = sections_dict.get("USAGE", "")
        data["dependencies"] = self._parse_dependencies_from_markdown(sections_dict.get("DEPENDENCIES", ""))
        data["related"] = self._parse_related(sections_dict.get("RELATED", ""))
        data["callouts"] = self._parse_callouts(sections_dict.get("CALLOUTS", "")) # Assuming callouts are directly parsed.

        # Ensure defaults for fields that might not be in every document
        data.setdefault("filename", "")
        data.setdefault("tags", [])
        data.setdefault("type", "")
        data.setdefault("summary", "")
        data.setdefault("details", "")
        data.setdefault("key_functions", [])
        data.setdefault("dependencies", [])
        data.setdefault("usage", "")
        data.setdefault("related", [])
        data.setdefault("callouts", [])

        logger.debug("Finished parsing markdown content.") # ADD LOGGING
        return data

    def _parse_tags_from_frontmatter(self, tags_str: str) -> List[str]:
        """Parses comma-separated tags string from frontmatter into a list."""
        if not tags_str:
            return []
        tags = [tag.strip() for tag in tags_str.split(',') if tag.strip()]
        logger.debug("Parsed tags from frontmatter: %s", tags) # ADD LOGGING
        return tags
    
    def _parse_dependencies_from_markdown(self, content: str) -> List[str]:
        """Parses backticked, comma-separated dependencies from markdown into a list."""
        if not content:
            return []
        # Remove backticks and split by comma
        cleaned_content = content.replace('`', '').strip()
        deps = [dep.strip() for dep in cleaned_content.split(',') if dep.strip()]
        logger.debug("Parsed dependencies from markdown: %s", deps) # ADD LOGGING
        return deps

    def _parse_dependencies(self, content: str) -> List[str]:
        """Parse comma-separated dependencies into a list."""
        if not content or content.lower() == 'none':
            return []
        deps = [dep.strip() for dep in content.split(',') if dep.strip()]
        logger.debug("Parsed dependencies: %s", deps) # ADD LOGGING
        return deps

    def _split_into_sections(self, raw_text: str) -> Dict[str, str]:
        """Splits the text into a dictionary of sections."""
        logger.debug("Splitting raw text into sections.") # ADD LOGGING
        # Pre-process to remove horizontal rules
        processed_text = re.sub(r'^\s*---\s*$', '', raw_text, flags=re.MULTILINE)
        
        sections = {}
        pattern = r"###\s*(?:\*\*)?(?P<name>[A-Z_]+)(?:\*\*)?\s*(?:###)?\s*(?P<content>.*?)(?=\n###|\Z)"
        matches = re.finditer(pattern, processed_text, re.DOTALL)
        for match in matches:
            name = match.group("name").strip()
            content = match.group("content").strip()
            sections[name] = content
            logger.debug("Extracted section: %s", name) # ADD LOGGING
        return sections

    def _parse_summary(self, content: str) -> str:
        return content

    def _parse_details(self, content: str) -> str:
        return content

    def _parse_usage(self, content: str) -> str:
        return content
        
    def _parse_type(self, content: str) -> str:
        return content.strip()

    def _parse_tags(self, content: str) -> List[str]:
        """Parses tags into a list (supports comma-separated or one-per-line)."""
        if not content:
            return []
        
        # Split by newline first, then by comma to support both formats
        raw_tags = []
        for line in content.split('\n'):
            for tag in line.split(','):
                cleaned_tag = tag.strip()
                if cleaned_tag.startswith('#'):
                    raw_tags.append(cleaned_tag)
        
        logger.debug("Parsed tags: %s", raw_tags) # ADD LOGGING
        return raw_tags

    def _parse_key_functions(self, content: str) -> List[Dict[str, str]]:
        """Parses a bulleted list of key functions."""
        functions = []
        if not content:
            return functions
        
        pattern = r"-\s*\**(?P<name>.*?)\**:\s*(?P<purpose>.*)"
        matches = re.finditer(pattern, content)
        for match in matches:
            functions.append({
                "name": match.group("name").strip(),
                "purpose": match.group("purpose").strip()
            })
        logger.debug("Parsed key functions: %s", functions) # ADD LOGGING
        return functions

    def _parse_related(self, content: str) -> List[str]:
        """Parses comma-separated wikilinks into a list."""
        if not content or content.lower() == 'none':
            return []
        
        # Remove [[ and ]] from the links
        links = [link.strip().replace('[[', '').replace(']]', '') for link in content.split(',') if link.strip()]
        logger.debug("Parsed related links: %s", links) # ADD LOGGING
        return links

    def _parse_callouts(self, content: str) -> List[str]:
        """Parses callouts from raw text."""
        if not content:
            return []
        
        callouts = []
        current_callout = []
        for line in content.split('\n'):
            if line.startswith('>[!'):
                if current_callout:
                    callouts.append('\n'.join(current_callout))
                current_callout = [line]
            elif current_callout:
                current_callout.append(line)
        
        if current_callout:
            callouts.append('\n'.join(current_callout))
        logger.debug("Parsed callouts: %s", callouts) # ADD LOGGING
        return callouts

    def parse_or_fallback(self, raw_response: str, filename: str) -> Dict[str, Any]:
        """
        Parses the text or returns a fallback structure on failure.
        """
        try:
            parsed_data = self.parse(raw_response, filename) # Pass filename to parse
            # The 'filename' is now handled within the parse method directly from the argument.
            if not all([parsed_data.get("summary"), parsed_data.get("tags"), parsed_data.get("type")]) :
                 raise ValueError("Missing one or more required sections: SUMMARY, TAGS, TYPE.")
            logger.info("Successfully parsed raw response for %s", filename) # ADD LOGGING
            return parsed_data
        except Exception as e:
            logger.error("Error parsing raw response for %s: %s", filename, e, exc_info=True) # ADD LOGGING
            # Create fallback structure
            return {
                "filename": filename,
                "tags": ["#parsing-failed", "#needs-review"],
                "type": "code-notes",
                "summary": "Documentation generation failed - text parsing error",
                "details": f"Error: {e}",
                "key_functions": [],
                "dependencies": [],
                "usage": "Manual review required",
                "related": [],
                "callouts": [
                    f"> [!WARNING]- Text Parsing Failed\n> {e}\n\n**Raw output (first 500 chars):**\n```text\n{raw_response[:500]}\n```"
                ]
            }
