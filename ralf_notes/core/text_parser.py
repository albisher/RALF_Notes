import re
import yaml
from typing import Dict, Any, List, Tuple

class TextParser:
    """
    Parses the LLM's structured text output into a dictionary.
    """

    def parse(self, raw_text: str) -> Dict[str, Any]:
        """
        Parses the raw text and returns a dictionary.
        """
        sections = self._split_into_sections(raw_text)
        
        parsed_data = {
            "filename": self._parse_filename(sections.get("FILENAME", "")),
            "summary": self._parse_summary(sections.get("SUMMARY", "")),
            "tags": self._parse_tags(sections.get("TAGS", "")),
            "type": self._parse_type(sections.get("TYPE", "")),
            "key_functions": self._parse_key_functions(sections.get("KEY_FUNCTIONS", "")),
            "details": self._parse_details(sections.get("DETAILS", "")),
            "dependencies": self._parse_dependencies(sections.get("DEPENDENCIES", "")),
            "usage": self._parse_usage(sections.get("USAGE", "")),
            "related": self._parse_related(sections.get("RELATED", "")),
            "callouts": self._parse_callouts(sections.get("CALLOUTS", "")),
        }

        return parsed_data

    def parse_markdown(self, markdown_content: str) -> Dict[str, Any]:
        """
        Parses a Markdown document (typically generated by NoteFormatter)
        back into a structured dictionary.
        """
        data = {}
        
        # 1. Extract YAML Frontmatter
        frontmatter_match = re.match(r'^---\s*\n(.*?)\n---\s*\n', markdown_content, re.DOTALL)
        if frontmatter_match:
            frontmatter_str = frontmatter_match.group(1)
            try:
                frontmatter = yaml.safe_load(frontmatter_str)
                if frontmatter:
                    tags_str = frontmatter.get('tags', '')
                    data['tags'] = self._parse_tags_from_frontmatter(tags_str)
                    data['type'] = frontmatter.get('type', '')
                    # Created date is not part of the LLM output data structure, so we don't parse it back
            except yaml.YAMLError as e:
                # Log this warning properly later
                print(f"WARNING: Error parsing YAML frontmatter: {e}")
            markdown_content = markdown_content[frontmatter_match.end():]

        # 2. Extract H1 Header as filename
        h1_match = re.match(r'^\s*#\s*(.*?)\s*\n', markdown_content)
        if h1_match:
            data['filename'] = h1_match.group(1).strip()
            markdown_content = markdown_content[h1_match.end():]
        
        # 3. Extract other sections using a similar approach to _split_into_sections
        # Modify the pattern to look for ## headers instead of ### for main sections
        sections_dict = {}
        # Pattern for ## headers and ### headers for key functions
        pattern = r'^\s*##\s*(?P<name>[A-Za-z\s]+)\s*\n(?P<content>.*?)(?=\n\s*##\s*|\Z)'
        matches = re.finditer(pattern, markdown_content, re.DOTALL | re.MULTILINE)
        for match in matches:
            name = match.group('name').strip().replace(' ', '_').upper()
            content = match.group('content').strip()
            sections_dict[name] = content
        
        # Manually extract Key Functions because they use ### headers within ## Key Functions
        key_functions_content = sections_dict.get("KEY_FUNCTIONS", "")
        if key_functions_content:
            data["key_functions"] = self._parse_key_functions(key_functions_content)

        data["summary"] = sections_dict.get("SUMMARY", "")
        data["details"] = sections_dict.get("DETAILS", "")
        data["usage"] = sections_dict.get("USAGE", "")
        data["dependencies"] = self._parse_dependencies_from_markdown(sections_dict.get("DEPENDENCIES", ""))
        data["related"] = self._parse_related(sections_dict.get("RELATED", ""))
        data["callouts"] = self._parse_callouts(sections_dict.get("CALLOUTS", "")) # Assuming callouts are directly parsed.

        # Ensure defaults for fields that might not be in every document
        data.setdefault("filename", "")
        data.setdefault("tags", [])
        data.setdefault("type", "")
        data.setdefault("summary", "")
        data.setdefault("details", "")
        data.setdefault("key_functions", [])
        data.setdefault("dependencies", [])
        data.setdefault("usage", "")
        data.setdefault("related", [])
        data.setdefault("callouts", [])

        return data

    def _parse_tags_from_frontmatter(self, tags_str: str) -> List[str]:
        """Parses comma-separated tags string from frontmatter into a list."""
        if not tags_str:
            return []
        return [tag.strip() for tag in tags_str.split(',') if tag.strip()]
    
    def _parse_dependencies_from_markdown(self, content: str) -> List[str]:
        """Parses backticked, comma-separated dependencies from markdown into a list."""
        if not content:
            return []
        # Remove backticks and split by comma
        cleaned_content = content.replace('`', '').strip()
        return [dep.strip() for dep in cleaned_content.split(',') if dep.strip()]

    def _parse_filename(self, content: str) -> str:
        """Extract filename."""
        return content.strip()

    def _parse_dependencies(self, content: str) -> List[str]:
        """Parse comma-separated dependencies into a list."""
        if not content or content.lower() == 'none':
            return []
        return [dep.strip() for dep in content.split(',') if dep.strip()]

    def _split_into_sections(self, raw_text: str) -> Dict[str, str]:
        """Splits the text into a dictionary of sections."""
        # Pre-process to remove horizontal rules
        processed_text = re.sub(r'^\s*---\s*$', '', raw_text, flags=re.MULTILINE)
        
        sections = {}
        pattern = r"###\s*(?:\*\*)?(?P<name>[A-Z_]+)(?:\*\*)?\s*(?:###)?\s*(?P<content>.*?)(?=\n###|\Z)"
        matches = re.finditer(pattern, processed_text, re.DOTALL)
        for match in matches:
            name = match.group("name").strip()
            content = match.group("content").strip()
            sections[name] = content
        return sections

    def _parse_summary(self, content: str) -> str:
        return content

    def _parse_details(self, content: str) -> str:
        return content

    def _parse_usage(self, content: str) -> str:
        return content
        
    def _parse_type(self, content: str) -> str:
        return content.strip()

    def _parse_tags(self, content: str) -> List[str]:
        """Parses comma-separated tags into a list."""
        if not content:
            return []
        return [tag.strip() for tag in content.split(',') if tag.strip().startswith('#')]

    def _parse_key_functions(self, content: str) -> List[Dict[str, str]]:
        """Parses a bulleted list of key functions."""
        functions = []
        if not content:
            return functions
        
        pattern = r"-\s*\**(?P<name>.*?)\**:\s*(?P<purpose>.*)"
        matches = re.finditer(pattern, content)
        for match in matches:
            functions.append({
                "name": match.group("name").strip(),
                "purpose": match.group("purpose").strip()
            })
        return functions

    def _parse_related(self, content: str) -> List[str]:
        """Parses comma-separated wikilinks into a list."""
        if not content or content.lower() == 'none':
            return []
        
        # Remove [[ and ]] from the links
        links = [link.strip().replace('[[', '').replace(']]', '') for link in content.split(',') if link.strip()]
        return links

    def _parse_callouts(self, content: str) -> List[str]:
        """Parses callouts from raw text."""
        if not content:
            return []
        
        callouts = []
        current_callout = []
        for line in content.split('\n'):
            if line.startswith('>[!'):
                if current_callout:
                    callouts.append('\n'.join(current_callout))
                current_callout = [line]
            elif current_callout:
                current_callout.append(line)
        
        if current_callout:
            callouts.append('\n'.join(current_callout))
            
        return callouts

    def parse_or_fallback(self, raw_response: str, filename: str) -> Dict[str, Any]:
        """
        Parses the text or returns a fallback structure on failure.
        """
        try:
            parsed_data = self.parse(raw_response)
            # Override filename with provided value (trust file system, not LLM)
            # parsed_data["filename"] = filename # Removed manual override
            if not all([parsed_data.get("summary"), parsed_data.get("tags"), parsed_data.get("type")]) :
                 raise ValueError("Missing one or more required sections: SUMMARY, TAGS, TYPE.")
            return parsed_data
        except Exception as e:
            # Create fallback structure
            return {
                "filename": filename,
                "tags": ["#parsing-failed", "#needs-review"],
                "type": "code-notes",
                "summary": "Documentation generation failed - text parsing error",
                "details": f"Error: {e}",
                "key_functions": [],
                "dependencies": [],
                "usage": "Manual review required",
                "related": [],
                "callouts": [
                    f"> [!WARNING]- Text Parsing Failed\n> {e}\n\n**Raw output (first 500 chars):**\n```text\n{raw_response[:500]}\n```"
                ]
            }
