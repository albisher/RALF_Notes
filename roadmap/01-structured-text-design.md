# Structured Text Format Design

**Date:** 2026-01-09 (Updated: 2026-01-10)
**Purpose:** Define the 3-stage structured text approach that replaced 9 separate generators
**Status:** âœ… Implemented and Production

---

## Overview

The V2 architecture uses a **3-stage pipeline** with structured text format (not JSON) to generate documentation in a single LLM call.

**Key Innovation:** Instead of JSON, we use a simpler structured text format with section headers that's easier for LLMs to generate reliably.

---

## ğŸ¯ The 3-Stage Approach

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 1: StructuredTextGenerator                            â”‚
â”‚ Input:  File content                                        â”‚
â”‚ Output: Structured text with section headers               â”‚
â”‚ Tool:   LLM (Ollama)                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 2: TextParser                                         â”‚
â”‚ Input:  Raw structured text from LLM                        â”‚
â”‚ Output: Python dictionary                                   â”‚
â”‚ Tool:   Regex-based parser                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 3: NoteFormatter                                      â”‚
â”‚ Input:  Python dictionary                                   â”‚
â”‚ Output: Obsidian markdown                                   â”‚
â”‚ Tool:   String formatting                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Stage 1: Structured Text Format

### Format Specification

The LLM generates structured text with section headers:

```
### FILENAME
<filename_without_extension>

### TAGS
#tag1, #tag2, #tag3

### TYPE
code-notes

### SUMMARY
Brief 1-2 sentence description of what this code does.

### DETAILS
More detailed explanation of logic and data flow in 2-4 sentences.

### KEY_FUNCTIONS
- **function_name**: Purpose of the function.
- **another_function**: Another purpose.

### DEPENDENCIES
dependency1, dependency2, dependency3

### USAGE
How to use this code/system.

### RELATED
[[Related Document 1]], [[Related Document 2]]

### CALLOUTS
>[!INFO]- Important Note
> Detailed explanation.
```

### Section Definitions

| Section | Required | Format | Description |
|---------|----------|--------|-------------|
| **FILENAME** | âœ… | Plain text | Filename without extension |
| **TAGS** | âœ… | Comma-separated | At least 2, max 10, starting with # |
| **TYPE** | âœ… | Single value | Document type classification |
| **SUMMARY** | âœ… | 1-2 sentences | High-level purpose (20-500 chars) |
| **DETAILS** | âœ… | 2-4 sentences | Logic and data flow explanation |
| **KEY_FUNCTIONS** | Optional | Bulleted list | Important functions/classes with purposes |
| **DEPENDENCIES** | Optional | Comma-separated | External libraries or modules |
| **USAGE** | Optional | Free text | How to use the code |
| **RELATED** | Optional | Wikilinks | Related Obsidian documents |
| **CALLOUTS** | Optional | Callout blocks | Important notes/warnings |

### Valid TYPE Values

- `code-notes` - Source code documentation
- `documentation` - General documentation
- `research` - Research notes
- `test-reference` - Test documentation
- `configuration` - Config file documentation
- `api-reference` - API documentation
- `architecture` - Architecture documentation
- `tutorial` - Tutorial/guide

### System Prompt

**Location:** `ralf_notes/core/schema.py`

```python
UNIFIED_SYSTEM_PROMPT = '''Analyze this code file. Return ONLY the following structured text format.

EXACT FORMAT:

###FILENAME
<filename_without_extension_only>

###TAGS
#<tag1>, #<tag2>, #<tag3> (at least 2, max 10, comma-separated, starting with #)

###TYPE
<document_type> (e.g., code-notes, documentation, research, test-reference, configuration, api-reference, architecture, tutorial)

###SUMMARY
<1-2 sentence high-level purpose, min 20 chars, max 500 chars>

###DETAILS
<2-4 sentences explaining logic and data flow>

###KEY_FUNCTIONS
- **function_name**: Purpose of the function.
- **another_function**: Another purpose. (Important functions, classes, or components, each on a new line)

###DEPENDENCIES
<dependency1>, <dependency2> (comma-separated list of external libraries or modules)

###USAGE
<How to use this code/system>

###RELATED
[[<Related Document 1>]], [[<Related Document 2>]] (Obsidian wikilinks to related documents, comma-separated, use 'none' if none)

###CALLOUTS
>[!INFO]- Important Note
> <Detailed explanation for the callout.>
>[!WARNING]- Caution
> <Another important callout.> (Obsidian callout blocks, each on a new line)

CRITICAL: Adhere strictly to the exact format, especially section headers like ###SECTION_NAME. Do not include any JSON. Do not include any additional markdown elements outside of the specified format.
'''
```

### Example LLM Output

```
### FILENAME
document_pipeline

### TAGS
#core, #pipeline, #documentation, #orchestration

### TYPE
code-notes

### SUMMARY
Orchestrates the complete document generation flow from file input to formatted Obsidian markdown output.

### DETAILS
The DocumentPipeline class coordinates three components: StructuredTextGenerator for LLM interaction, TextParser for parsing structured text, and NoteFormatter for markdown formatting. It handles the complete flow including error handling and metadata tracking.

### KEY_FUNCTIONS
- **generate_document**: Main entry point that orchestrates the full generation pipeline.
- **__init__**: Initializes pipeline with all required components via dependency injection.

### DEPENDENCIES
pathlib, typing, ollama

### USAGE
Create a DocumentPipeline instance with injected dependencies, then call generate_document(file_path) to process files. Returns tuple of (markdown_content, metadata).

### RELATED
[[StructuredTextGenerator]], [[TextParser]], [[NoteFormatter]]

### CALLOUTS
>[!INFO]- Error Handling
> Pipeline provides graceful error handling with fallback markdown generation on failures.
```

---

## Stage 2: Text Parser

### Purpose

Parse structured text from LLM into a Python dictionary.

**Location:** `ralf_notes/core/text_parser.py`

### Parsing Strategy

```python
class TextParser:
    """
    Box: Text Parser

    Input: Raw LLM text with section headers
    Output: Parsed dictionary
    Responsibility: Extract sections using regex patterns
    """

    def parse(self, raw_text: str) -> Dict[str, Any]:
        """Parse structured text into dictionary."""
        # 1. Split into sections using regex
        sections = self._split_into_sections(raw_text)

        # 2. Parse each section with type-specific parser
        return {
            "filename": self._parse_filename(sections.get("FILENAME", "")),
            "tags": self._parse_tags(sections.get("TAGS", "")),
            "type": self._parse_type(sections.get("TYPE", "")),
            "summary": self._parse_summary(sections.get("SUMMARY", "")),
            "details": self._parse_details(sections.get("DETAILS", "")),
            "key_functions": self._parse_key_functions(sections.get("KEY_FUNCTIONS", "")),
            "dependencies": self._parse_dependencies(sections.get("DEPENDENCIES", "")),
            "usage": self._parse_usage(sections.get("USAGE", "")),
            "related": self._parse_related(sections.get("RELATED", "")),
            "callouts": self._parse_callouts(sections.get("CALLOUTS", "")),
        }
```

### Section Parsing

Each section has a dedicated parser:

```python
def _split_into_sections(self, raw_text: str) -> Dict[str, str]:
    """Split text by ### SECTION_NAME headers."""
    pattern = r"###\s*(?P<name>[A-Z_]+)\s*(?P<content>.*?)(?=\n###|\Z)"
    matches = re.finditer(pattern, raw_text, re.DOTALL)
    return {match.group("name"): match.group("content").strip()
            for match in matches}

def _parse_tags(self, content: str) -> List[str]:
    """Parse comma-separated tags."""
    if not content:
        return []
    return [tag.strip() for tag in content.split(',')
            if tag.strip().startswith('#')]

def _parse_key_functions(self, content: str) -> List[Dict[str, str]]:
    """Parse bulleted function list."""
    pattern = r"-\s*\**(?P<name>.*?)\**:\s*(?P<purpose>.*)"
    return [{"name": m.group("name").strip(),
             "purpose": m.group("purpose").strip()}
            for m in re.finditer(pattern, content)]

def _parse_dependencies(self, content: str) -> List[str]:
    """Parse comma-separated dependencies."""
    if not content or content.lower() == 'none':
        return []
    return [dep.strip() for dep in content.split(',') if dep.strip()]
```

### Fallback Mechanism

If parsing fails, return a fallback structure:

```python
def parse_or_fallback(self, raw_response: str, filename: str) -> Dict[str, Any]:
    """Parse or return fallback structure on failure."""
    try:
        parsed_data = self.parse(raw_response)
        parsed_data["filename"] = filename

        # Validate required fields
        if not all([parsed_data.get("summary"),
                    parsed_data.get("tags"),
                    parsed_data.get("type")]):
            raise ValueError("Missing required sections")

        return parsed_data
    except Exception as e:
        # Return fallback structure
        return {
            "filename": filename,
            "tags": ["#parsing-failed", "#needs-review"],
            "type": "code-notes",
            "summary": "Documentation generation failed - text parsing error",
            "details": f"Error: {e}",
            "key_functions": [],
            "dependencies": [],
            "usage": "Manual review required",
            "related": [],
            "callouts": [
                f"> [!WARNING]- Text Parsing Failed\n> {e}\n\n"
                f"**Raw output (first 500 chars):**\n```text\n{raw_response[:500]}\n```"
            ]
        }
```

---

## Stage 3: Note Formatter

### Purpose

Convert parsed dictionary into Obsidian-formatted markdown.

**Location:** `ralf_notes/core/note_formatter.py`

### Formatting Strategy

```python
class NoteFormatter:
    """
    Box: Note Formatter

    Input: Parsed dictionary
    Output: Obsidian markdown
    Responsibility: Format data as beautiful Obsidian markdown
    """

    def format(self, data: Dict[str, Any]) -> str:
        """Convert dictionary to markdown."""
        sections = [
            self._format_frontmatter(data),
            self._format_header(data),
            self._format_summary(data),
            self._format_details(data),
            self._format_key_functions(data),
            self._format_usage(data),
            self._format_dependencies(data),
            self._format_related(data),
            self._format_callouts(data),
        ]

        # Filter out empty sections
        return '\n\n'.join(s for s in sections if s).strip() + '\n'
```

### Section Formatters

Each section has a dedicated formatter:

```python
def _format_frontmatter(self, data: Dict[str, Any]) -> str:
    """Generate YAML frontmatter."""
    date = datetime.now().strftime("%Y-%m-%d")
    tags = ', '.join(data.get('tags', ['#documentation']))
    doc_type = data.get('type', 'code-notes')

    return f"""---
tags: {tags}
created: {date}
type: {doc_type}
---"""

def _format_header(self, data: Dict[str, Any]) -> str:
    """Generate H1 header."""
    return f"# {data.get('filename', 'Untitled')}"

def _format_summary(self, data: Dict[str, Any]) -> Optional[str]:
    """Generate Summary section."""
    summary = data.get('summary', '')
    if not summary:
        return None
    return f"## Summary\n\n{summary}"

def _format_key_functions(self, data: Dict[str, Any]) -> Optional[str]:
    """Generate Key Functions section."""
    functions = data.get('key_functions', [])
    if not functions:
        return None

    content = "## Key Functions\n\n"
    for func in functions:
        name = func.get('name', 'unknown')
        purpose = func.get('purpose', 'No description')
        content += f"### `{name}`\n\n{purpose}\n\n"

    return content.strip()

def _format_dependencies(self, data: Dict[str, Any]) -> Optional[str]:
    """Generate Dependencies section."""
    deps = data.get('dependencies', [])
    if not deps:
        return None

    deps_list = ', '.join(f"`{dep}`" for dep in deps)
    return f"## Dependencies\n\n{deps_list}"
```

### Example Output

```markdown
---
tags: #core, #pipeline, #documentation, #orchestration
created: 2026-01-10
type: code-notes
---

# document_pipeline

## Summary

Orchestrates the complete document generation flow from file input to formatted Obsidian markdown output.

## Details

The DocumentPipeline class coordinates three components: StructuredTextGenerator for LLM interaction, TextParser for parsing structured text, and NoteFormatter for markdown formatting. It handles the complete flow including error handling and metadata tracking.

## Key Functions

### `generate_document`

Main entry point that orchestrates the full generation pipeline.

### `__init__`

Initializes pipeline with all required components via dependency injection.

## Usage

Create a DocumentPipeline instance with injected dependencies, then call generate_document(file_path) to process files. Returns tuple of (markdown_content, metadata).

## Dependencies

`pathlib`, `typing`, `ollama`

## Related

- [[StructuredTextGenerator]]
- [[TextParser]]
- [[NoteFormatter]]

>[!INFO]- Error Handling
> Pipeline provides graceful error handling with fallback markdown generation on failures.
```

---

## Why Structured Text > JSON?

### Advantages of Structured Text

1. **Simpler for LLMs** - Text format is more natural than JSON syntax
2. **No escaping issues** - Don't need to escape quotes, braces, etc.
3. **More forgiving** - Parser can handle slight format variations
4. **Easier debugging** - Human-readable without tools
5. **Robust fallbacks** - Can parse partial outputs

### JSON Challenges (Why We Changed)

1. âŒ LLMs sometimes include markdown code blocks around JSON
2. âŒ JSON requires perfect syntax (one missing comma breaks everything)
3. âŒ Nested structures need careful escaping
4. âŒ Hard to recover from partial/malformed output
5. âŒ Control characters in strings break parsing

### Structured Text Solutions

1. âœ… No code blocks needed - plain text format
2. âœ… Sections are independent - one bad section doesn't break others
3. âœ… Simple patterns - just look for ### SECTION_NAME
4. âœ… Partial parsing possible - can extract what exists
5. âœ… No special character escaping needed

---

## Implementation Details

### File Locations

```
ralf_notes/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ schema.py                    # Structured text format definition
â”‚   â”œâ”€â”€ structured_text_generator.py # Stage 1: LLM generation
â”‚   â”œâ”€â”€ text_parser.py               # Stage 2: Parsing
â”‚   â”œâ”€â”€ note_formatter.py            # Stage 3: Formatting
â”‚   â””â”€â”€ document_pipeline.py         # Orchestrates all 3 stages
```

### Data Flow

```python
# Stage 1: Generate structured text
generator = StructuredTextGenerator(client, config)
raw_text = generator.generate(context)
# Output: "### FILENAME\ndocument_pipeline\n### TAGS\n#core, #pipeline\n..."

# Stage 2: Parse into dictionary
parser = TextParser()
data = parser.parse(raw_text)
# Output: {"filename": "document_pipeline", "tags": ["#core", "#pipeline"], ...}

# Stage 3: Format as markdown
formatter = NoteFormatter()
markdown = formatter.format(data)
# Output: "---\ntags: #core, #pipeline\n---\n\n# document_pipeline\n..."
```

---

## Validation & Error Handling

### Parse Validation

```python
def parse_or_fallback(self, raw_response: str, filename: str) -> Dict[str, Any]:
    """Parse with validation and fallback."""
    try:
        parsed_data = self.parse(raw_response)
        parsed_data["filename"] = filename

        # Validate required fields exist
        required = ["summary", "tags", "type"]
        missing = [f for f in required if not parsed_data.get(f)]
        if missing:
            raise ValueError(f"Missing required sections: {missing}")

        return parsed_data
    except Exception as e:
        # Return safe fallback structure
        return self._create_fallback(filename, str(e), raw_response)
```

### Format Validation

```python
def format(self, data: Dict[str, Any]) -> str:
    """Format with safe defaults."""
    # Each formatter checks if data exists before formatting
    # Missing data = skip section (don't error)
    sections = [
        self._format_frontmatter(data),        # Always included
        self._format_header(data),             # Always included
        self._format_summary(data),            # Skip if missing
        self._format_details(data),            # Skip if missing
        # ... etc
    ]

    # Filter out None/empty sections
    return '\n\n'.join(s for s in sections if s)
```

---

## Configuration

### Generator Config

```python
@dataclass
class StructuredTextGeneratorConfig:
    """Configuration for text generation."""
    model_name: str = 'ministral-3:3b'
    num_ctx: int = 10000
    temperature: float = 0.1
    chunk_size: int = 100000
    max_content_length: int = 8000
    max_chunk_summary_length: int = 4000
    ollama_host: str = 'http://127.0.0.1:11434'
```

### LLM Settings

- **Model:** `ministral-3:3b` (fast, good quality)
- **Temperature:** `0.1` (low for consistency)
- **Context:** `10000` tokens
- **Format:** Structured text (not JSON)

---

## Performance Characteristics

### Single LLM Call

- **Old approach:** 9 LLM calls per file (~15s)
- **New approach:** 1 LLM call per file (~2s)
- **Speedup:** 7-9x faster

### Parsing Performance

- **Text parsing:** <1ms per file
- **Formatting:** <1ms per file
- **Total overhead:** Negligible compared to LLM time

### Success Rate

- **Parse success:** >95% (with fallback: 100%)
- **Format success:** 100% (always produces valid markdown)
- **Overall reliability:** Very high

---

## Testing Strategy

### Unit Tests

```python
def test_parse_complete_output():
    """Test parsing complete structured text."""
    raw_text = """### FILENAME
test_file

### TAGS
#test, #example

### TYPE
code-notes

### SUMMARY
This is a test summary."""

    parser = TextParser()
    result = parser.parse(raw_text)

    assert result["filename"] == "test_file"
    assert result["tags"] == ["#test", "#example"]
    assert result["type"] == "code-notes"
    assert "test summary" in result["summary"]

def test_parse_fallback():
    """Test fallback on malformed input."""
    parser = TextParser()
    result = parser.parse_or_fallback("invalid text", "test_file")

    assert result["filename"] == "test_file"
    assert "#parsing-failed" in result["tags"]
    assert result["valid"] == False
```

### Integration Tests

```python
def test_full_pipeline():
    """Test complete 3-stage pipeline."""
    # Stage 1: Generate
    generator = StructuredTextGenerator(client, config)
    raw_text = generator.generate(context)

    # Stage 2: Parse
    parser = TextParser()
    data = parser.parse_or_fallback(raw_text, "test")

    # Stage 3: Format
    formatter = NoteFormatter()
    markdown = formatter.format(data)

    # Verify
    assert markdown.startswith("---")  # Has frontmatter
    assert "# test" in markdown         # Has header
    assert "## Summary" in markdown     # Has sections
```

---

## Migration Notes

### From JSON to Structured Text

**What Changed:**
- âœ… Format: JSON â†’ Structured text with section headers
- âœ… Parser: JSONExtractor â†’ TextParser
- âœ… Approach: Single JSON object â†’ Section-based text
- âœ… Reliability: ~85% â†’ >95% parse success

**What Stayed:**
- âœ… Single LLM call (1 per file)
- âœ… 3-stage architecture
- âœ… Fallback mechanisms
- âœ… Error handling
- âœ… Performance characteristics

---

## Comparison Table

| Aspect | JSON Approach | Structured Text | Winner |
|--------|--------------|-----------------|---------|
| **Parse Success** | ~85% | >95% | âœ… Text |
| **LLM Reliability** | Medium | High | âœ… Text |
| **Error Recovery** | Difficult | Easy | âœ… Text |
| **Debugging** | Harder | Easier | âœ… Text |
| **Code Complexity** | Higher | Lower | âœ… Text |
| **Type Safety** | Native | Manual | âŒ JSON |
| **Validation** | Schema-based | Pattern-based | âŒ JSON |

**Overall:** Structured text is simpler and more reliable for LLM output.

---

## Future Enhancements

### Possible Improvements

1. **Schema validation** - Add formal schema for text format
2. **Partial updates** - Support updating individual sections
3. **Multi-language** - Support for different output languages
4. **Custom sections** - Allow user-defined sections
5. **Streaming parsing** - Parse as LLM generates

### Backwards Compatibility

If needed, could support both formats:
```python
def parse_response(raw: str, filename: str):
    """Parse JSON or structured text."""
    if raw.strip().startswith('{'):
        return parse_json(raw, filename)
    else:
        return parse_text(raw, filename)
```

---

## Conclusion

The structured text approach provides:

âœ… **Simplicity** - Easier for LLMs to generate
âœ… **Reliability** - >95% parse success rate
âœ… **Performance** - 9x faster than old approach
âœ… **Maintainability** - Cleaner, simpler code
âœ… **Flexibility** - Easy to extend with new sections

This 3-stage architecture (Generate â†’ Parse â†’ Format) is the foundation of RALF Notes V2 and has proven to be robust and production-ready.

---

**Document Version:** 2.0
**Date:** 2026-01-10
**Status:** Current Implementation
**Related:** [02-architecture-refactoring.md](02-architecture-refactoring.md), [schema.py](../ralf_notes/core/schema.py)
